{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3157,"status":"ok","timestamp":1685107098212,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"4gVjyAMrPfu3"},"outputs":[],"source":["import numpy as np\n","import torch \n","import matplotlib.pyplot as plt\n","import sklearn\n","import torchvision\n","from torchvision import datasets\n","import torch.nn as nn\n","from torch import optim\n","from torch.autograd import Variable\n","import pandas as pd\n","import random\n","from torchvision import transforms\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, random_split\n","from torchsummary import summary\n","from tqdm import tqdm\n","import wandb\n","import scipy.ndimage"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1685107098213,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"BVf_FffNwTDm"},"outputs":[],"source":["seq_len = 6"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HKfK3ugXPfvI"},"source":["# Feature extraction from images\n","\n","via encoder part of CNN --> extract features from spatially resolved image data via pre-implemented encoder part of convolutional autoencoder to get low-dimensional embedding\n","\n","1) reload pre-trained encoder part\n","2) feed every sequence through encoder to get low-dimensional embedding of each sequence\n","3) store low-dimensional embedding of each sequence again in tensor dataset that can serve as input to LSTM\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1685107098213,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"_3bJiyfkPfvJ"},"outputs":[],"source":["## Construction of encoder part\n","class Encoder_original(nn.Module):\n","    \n","    def __init__(self, encoded_space_dim,fc2_input_dim):\n","        super().__init__()\n","        \n","        ### Convolutional section\n","        self.encoder_cnn = nn.Sequential(\n","            nn.Conv2d(1, 8, 3, stride=2, padding=1),\n","            nn.ReLU(True),\n","            #nn.Dropout(p=0.2),\n","            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(True),\n","            #nn.Dropout(p=0.2),\n","            nn.Conv2d(16, 32, 3, stride=2, padding=0),\n","            nn.ReLU(True),\n","            #nn.Dropout(p=0.2)\n","        )\n","        \n","        ### Flatten layer\n","        self.flatten = nn.Flatten(start_dim=1)\n","### Linear section\n","        self.encoder_lin = nn.Sequential(\n","            nn.Linear(3 * 3 * 32, 128),\n","            nn.ReLU(True),\n","            #nn.Dropout(p=0.5),\n","            nn.Linear(128, encoded_space_dim)\n","        )\n","        \n","    def forward(self, x):\n","        x = self.encoder_cnn(x)\n","        x = self.flatten(x)\n","        x = self.encoder_lin(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4281,"status":"ok","timestamp":1685107102490,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"Wln8lhKlhqa4"},"outputs":[],"source":["dataset_train = torch.load(\"train_seq_data.pt\")\n","dataset_test = torch.load(\"test_seq_data.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1685107102490,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"zHMOClLCPfvR"},"outputs":[],"source":["trainloader = DataLoader(dataset=dataset_train, batch_size=128, shuffle=False)\n","testloader = DataLoader(dataset=dataset_test, batch_size=1024, shuffle=False)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hacw4yZRPfvR"},"source":["## Low-dimensional embedding with LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1685107102491,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"8_zbyMc5PfvS"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class Simple_LSTM(nn.Module):\n","    def __init__(self, lin_in, lin_out, lstm_in, lstm_hidden_size, lstm_num_layers, dropout_prob):\n","        super().__init__()\n","        self.linear = nn.Linear(lin_in, lin_out)\n","        self.activation = nn.ReLU()\n","        self.dropout = nn.Dropout( p=dropout_prob)\n","        self.lstm = nn.LSTM(lstm_in, lstm_hidden_size, lstm_num_layers, batch_first=True, bidirectional=False)\n","        \n","    def forward(self, x):\n","        # x has shape [batch_size, sequence_length, input_size]\n","        # pass x through the linear layer without losing the sequence structure\n","        x = self.linear(x)\n","        x = self.activation(x)\n","        x = self.dropout(x)\n","        \n","        # pass the output of the linear layer through the LSTM layer\n","        # the batch_first=True argument means that the input has shape [batch_size, sequence_length, input_size]\n","        # the LSTM layer outputs a tuple (output, (h_n, c_n)), but we only need the output\n","        lstm_output, lstm_hidden = self.lstm(x)\n","        \n","        # return the output of the LSTM layer\n","        return lstm_output, lstm_hidden\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"omD1-dLyPfvV"},"source":["## Prediction of next image in sequence with Decoder CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1685107102491,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"flECeDWRPfvV"},"outputs":[],"source":["## Construction of decoder part\n","class Decoder_original(nn.Module):\n","    \n","    def __init__(self, encoded_space_dim,fc2_input_dim):\n","        super().__init__()\n","        self.decoder_lin = nn.Sequential(\n","            nn.Linear(encoded_space_dim, 128),\n","            nn.ReLU(True),\n","            #nn.Dropout(p=0.5),\n","            nn.Linear(128, 3 * 3 * 32),\n","            nn.ReLU(True),\n","            #nn.Dropout(p=0.5),\n","        )\n","\n","        self.unflatten = nn.Unflatten(dim=1, \n","        unflattened_size=(32, 3, 3))\n","\n","        self.decoder_conv = nn.Sequential(\n","            nn.ConvTranspose2d(32, 16, 3, \n","            stride=2, output_padding=0),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(16, 8, 3, stride=2, \n","            padding=1, output_padding=1),\n","            nn.BatchNorm2d(8),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(8, 1, 3, stride=2, \n","            padding=1, output_padding=1)\n","        )\n","        \n","    def forward(self, x):\n","        x = self.decoder_lin(x)\n","        x = self.unflatten(x)\n","        x = self.decoder_conv(x)\n","        x = torch.sigmoid(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1685107102492,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"PFJbzobJPfvZ"},"outputs":[],"source":["def plot_comparison(img_pred, img_orig):\n","    # Convert tensors to numpy arrays\n","    img_pred_np = img_pred.squeeze().detach().numpy()\n","    img_orig_np = img_orig.squeeze().detach().numpy()\n","\n","    # Plot images side-by-side\n","    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n","    ax[0].imshow(img_orig_np, cmap='gray')\n","    ax[0].set_title('Original')\n","    ax[1].imshow(img_pred_np, cmap='gray')\n","    ax[1].set_title('Predicted')\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1685107102493,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"V4zYMSzXPBwY"},"outputs":[],"source":["import random\n","import matplotlib.pyplot as plt\n","\n","def plot_random_predictions(conc_label, conc_target, conc_out):\n","    # Define a list to store the randomly chosen label values\n","    label_values = []\n","\n","    # Choose a random image for each of the 10 labels and store the label values\n","    for i in range(10):\n","        # Find all indices where the label tensor has the current label value\n","        indices = (conc_label == i).nonzero(as_tuple=True)[0]\n","\n","        # Choose a random index from the indices and extract the corresponding label value\n","        index = indices[random.randint(0, len(indices) - 1)]\n","        label_values.append(index)\n","\n","    # Define the figure and axes for the plot\n","    fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n","\n","    # Plot the original images in the top row\n","    for i, index in enumerate(label_values):\n","        # Find the first index where the label tensor has the current label value\n","        #index = (conc_label == label_value).nonzero(as_tuple=True)[0][0]\n","\n","        # Extract the corresponding original image tensor\n","        img_orig = conc_target[index]\n","\n","        # Convert the tensor to a numpy array and plot the image\n","        img_orig_np = img_orig.squeeze().detach().numpy()\n","        axes[0][i].imshow(img_orig_np, cmap='gray')\n","        axes[0][i].set_title(f'Label {i}')\n","\n","    # Plot the predicted images in the bottom row\n","    for i, index in enumerate(label_values):\n","        # Find the first index where the label tensor has the current label value\n","        #index = (conc_label == label_value).nonzero(as_tuple=True)[0][0]\n","\n","        # Extract the corresponding predicted image tensor\n","        img_pred = conc_out[index]\n","\n","        # Convert the tensor to a numpy array and plot the image\n","        img_pred_np = img_pred.squeeze().detach().numpy()\n","        axes[1][i].imshow(img_pred_np, cmap='gray')\n","\n","    # Set the title for the plot\n","    fig.suptitle('Randomly chosen original images and their corresponding predicted images')\n","\n","    # Show the plot\n","    plt.show()\n","    return fig\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1685107102493,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"wzGmgWz1Pfva"},"outputs":[],"source":["## Implementation of training and evaluation function"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1685107102494,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"V-1Cqi5Lq-CM"},"outputs":[],"source":["device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1685612311760,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"BDSEys4fPfva"},"outputs":[],"source":["def train_img_prediction(lstm, decoder, device, dataloader, loss_fn, optimizer):\n","    # Set train mode for both the encoder and the decoder\n","    lstm.train()\n","    decoder.train()\n","    #lstm.to(device)\n","    #decoder.to(device)\n","    train_loss = []\n","    #counter = 0\n","    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n","    for image_batch, target_batch, label, _ ,_, _,_ in dataloader: \n","        # Move tensor to the proper device\n","        #lstm.zero_grad()\n","        #decoder.zero_grad()\n","        image_batch = image_batch.to(device)\n","        target_batch = target_batch.to(device)\n","        # Encode data\n","        all_layers, last_layer = lstm(image_batch)\n","        h, c = last_layer\n","        #h_forward = h[::2, :,:]\n","        embedded_data = h[2]\n","        #embedded_data = h_forward[-1]\n","        \n","        # Decode data\n","        pred_img_batch = decoder(embedded_data)\n","        \n","        # Evaluate loss\n","        loss = loss_fn(pred_img_batch.float(), target_batch.float())\n","        #train_loss.append(loss.detach().cpu().numpy())\n","        \n","        # Backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        #lstm.zero_grad()\n","        #decoder.zero_grad()\n","        \n","        # Print batch loss\n","        #print('\\t partial train loss (single batch): %f' % (loss.data))\n","        train_loss.append(loss.detach().cpu().numpy())\n","        #counter += 1\n","        #if counter % 100 == 0:\n","          #print(\"pred_img: \",pred_img_batch[0])\n","          #print(\"last layer: \", embedded_data[0])\n","        \n","        \n","\n","    return np.mean(train_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1685612311760,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"4UMgak6sPfvb"},"outputs":[],"source":["## Evaluation of model on test dataset\n","def evaluate_img_prediction(lstm, decoder, device, dataloader, loss_fn):\n","    # Set evaluation mode for encoder and decoder\n","    lstm.eval()\n","    decoder.eval()\n","    with torch.no_grad(): # No need to track the gradients\n","        # Define the lists to store the outputs for each batch\n","        conc_out = []\n","        conc_target = []\n","        conc_label = []\n","        for image_batch, target_batch, label, _ ,_, _,_  in dataloader:\n","            # Move tensor to the proper device\n","            image_batch = image_batch.to(device)\n","            target_batch = target_batch.to(device)\n","            # Encode data\n","            all_layers, last_layer = lstm(image_batch)\n","            h, c = last_layer\n","            #h_forward = h[::2, :,:]\n","            embedded_data = h[2]\n","            #embedded_data = h_forward[-1]\n","            # Decode data\n","            pred_img_batch = decoder(embedded_data)\n","            # Append the network output and the original image to the lists\n","            conc_out.append(pred_img_batch.cpu())\n","            conc_target.append(target_batch.cpu())\n","            conc_label.append(label.cpu())\n","        # Create a single tensor with all the values in the lists\n","        conc_out = torch.cat(conc_out)\n","        conc_target = torch.cat(conc_target) \n","        conc_label = torch.cat(conc_label)\n","        # Evaluate global loss\n","    val_loss = loss_fn(conc_out.float(), conc_target.float())\n","\n","    # Choose a random index\n","    index = random.randint(0, len(conc_out) - 1)\n","\n","    # Extract corresponding img_pred and img_orig tensors\n","    img_pred = conc_out[index]\n","    img_orig = conc_target[index]\n","    #plot_comparison(img_pred, img_orig)\n","    rec_fig = plot_random_predictions(conc_out=conc_out, conc_target=conc_target, conc_label=conc_label)\n","\n","\n","    return val_loss, rec_fig\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3590,"status":"ok","timestamp":1685107106075,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"LaOELhchPfvc","outputId":"5ae468bc-3a4a-4ce1-c17c-5e5c4e5b4273"},"outputs":[],"source":["### Define the loss function\n","loss_fn = torch.nn.MSELoss()\n","\n","### Define an optimizer (both for the encoder and the decoder!)\n","lr= 0.001\n","\n","### Set the random seed for reproducible results\n","torch.manual_seed(0)\n","\n","### Initialize the two networks\n","d = 25\n","seq_len = 6\n","\n","### Define weight decay\n","weight_decay = 0\n","\n","### Define model parameters\n","lin_in = d\n","lin_out = 128\n","hidden_size = 25\n","stack = 3\n","\n","#model = Autoencoder(encoded_space_dim=encoded_space_dim)\n","lstm_model = Simple_LSTM(lin_in=d, lin_out=lin_out, lstm_in=lin_out, lstm_hidden_size=hidden_size, lstm_num_layers=stack, dropout_prob=0)\n","decoder_prediction = Decoder_original(encoded_space_dim=hidden_size,fc2_input_dim=128)\n","params_to_optimize = [\n","    {'params': lstm_model.parameters()},\n","    {'params': decoder_prediction.parameters()}\n","]\n","\n","optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=weight_decay)\n","\n","# Check if the GPU is available\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(f'Selected device: {device}')\n","#device = \"cpu\"\n","# Move both the encoder and the decoder to the selected device\n","decoder_prediction.to(device)\n","lstm_model.to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1038274,"status":"ok","timestamp":1685108147831,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"JhqhfNdMPfvd","outputId":"4f7f81df-c828-49ef-8b42-ae6242b3f748"},"outputs":[],"source":["num_epochs = 12 #number of iterations\n","diz_loss = {'train_loss':[],'val_loss':[]} #store training and evaluation loss\n","for epoch in range(num_epochs):\n","   train_loss = train_img_prediction(lstm=lstm_model,decoder=decoder_prediction,device=device,dataloader=trainloader, loss_fn=loss_fn, optimizer=optim)\n","   val_loss, _ = evaluate_img_prediction(lstm=lstm_model,decoder=decoder_prediction,device=device,dataloader=testloader, loss_fn=loss_fn) #evaluate perfomance of autoencoder on test set\n","   print('\\n EPOCH {}/{} \\t train loss {} \\t val loss {}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n","\n","   diz_loss['train_loss'].append(train_loss)\n","   diz_loss['val_loss'].append(val_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1685108147831,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"U3i3gST8sdVL","outputId":"415b4023-fe87-48b8-efe4-cfe205d4d74a"},"outputs":[],"source":["def plot_loss(diz_loss):\n","    train_loss = diz_loss['train_loss']\n","    val_loss = diz_loss['val_loss']\n","    epochs = range(1, len(train_loss) + 1)\n","\n","    fig = plt.figure()\n","    plt.plot(epochs, train_loss, 'g', label='Training loss')\n","    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","    plt.title('Training and Validation loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.show()\n","\n","    return fig\n","\n","\n","plt_loss = plot_loss(diz_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1685108147831,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"7CnU_-J2Ly5_","outputId":"22edbac7-6d62-4a0e-b81e-454c68820942"},"outputs":[],"source":["run.log({\"Plot_loss\": plt_loss})"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":298},"executionInfo":{"elapsed":1972,"status":"ok","timestamp":1685108149789,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"ehbwJD1iPfve","outputId":"5b0d21e0-21a8-4960-b299-8ab989ecef7f"},"outputs":[],"source":["val_loss, rec_fig = evaluate_img_prediction(lstm=lstm_model,decoder=decoder_prediction,device=device,dataloader=testloader, loss_fn=loss_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1685108149790,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"OtltpK6FMZjW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1685108149790,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"d_8rJiKxMyS_"},"outputs":[],"source":["run.log({\"Pred_Img\": rec_fig})"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1685108149790,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"AEovvdhx0vQQ"},"outputs":[],"source":["# Assume that your model is named \"model\" and your optimizer is named \"optimizer\"\n","lstm_state_dict = lstm_model.state_dict()\n","decoder_state_dict = decoder_prediction.state_dict()\n","optimizer_state_dict = optim.state_dict()\n","\n","# Combine the model and optimizer state dictionaries into a single dictionary\n","state_dict = {'lstm': lstm_state_dict,'decoder': decoder_state_dict, 'optimizer': optimizer_state_dict}\n","\n","# Specify the path where you want to save the state dictionary\n","PATH = \"/content/drive/MyDrive/Healing_MNIST/CNN_LSTM_const_seq/model_const_rand_rot_rand_square_UNIDIRECTIONAL.pth\"\n","\n","# Save the combined state dictionary to the specified path\n","torch.save(state_dict, PATH)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1685108149790,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"MiI8D6HCOvGi"},"outputs":[],"source":["def get_embeddings(data, device, lstm):\n","    encoded_samples = []\n","    for sample in tqdm(data):\n","        img = sample[0].to(device)\n","        label = sample[2]\n","        rot = sample[3]\n","        square = sample[4]\n","        square_count = sample[5]\n","        seq_len = sample[6]\n","        # Encode image\n","        lstm.eval()\n","        with torch.no_grad():\n","            all_layers, last_layer = lstm(img)\n","            h, c = last_layer\n","            #h_forward = h[::2, :,:]\n","            #embedded_img = h_forward[-1]\n","            embedded_img = h[-1]\n","        # Append to list\n","        encoded_img = embedded_img.cpu().numpy()\n","        encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(encoded_img)}\n","        encoded_sample['label'] = label.numpy()\n","        encoded_sample['square'] = square.numpy()\n","        encoded_sample['rotation'] = rot.numpy()\n","        encoded_sample['square_count'] = square_count.numpy()\n","        encoded_sample['seq_len'] = seq_len.numpy()\n","        encoded_samples.append(encoded_sample)\n","    encoded_samples = pd.DataFrame(encoded_samples)\n","    return encoded_samples"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37295,"status":"ok","timestamp":1685108187079,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"6dIG6DsWPDxd","outputId":"16f25737-bf69-4217-fd79-9cc46c9a48b4"},"outputs":[],"source":["train_embeddings = get_embeddings(data=dataset_train, device=device, lstm=lstm_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6148,"status":"ok","timestamp":1685108193214,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"KEbRqKpmPKap","outputId":"35ca86c2-38c4-40aa-feba-c9428b22be4a"},"outputs":[],"source":["test_embeddings = get_embeddings(data=dataset_test, device=device, lstm=lstm_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"elapsed":27254,"status":"ok","timestamp":1685108220455,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"8MWDr_7OPS5O","outputId":"5b35e7c0-f0b6-45f4-dca6-7679f3480fe8"},"outputs":[],"source":["from sklearn.manifold import TSNE\n","import plotly.io as pio\n","import plotly.express as px\n","\n","tsne = TSNE(n_components=2)\n","tsne_results = tsne.fit_transform(test_embeddings.drop(['label', 'square', 'rotation', 'seq_len', 'square_count'],axis=1))\n","fig_tsne_rot = px.scatter(tsne_results, x=0, y=1,\n","                 color=test_embeddings.rotation.astype(str),\n","                 symbol=test_embeddings.label.astype(str),\n","                 #symbol_sequence=['circle', 'cross'],\n","                 labels={'0': 'tsne-2d-one', '1': 'tsne-2d-two'})\n","fig_tsne_rot.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"elapsed":1169,"status":"ok","timestamp":1685108221607,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"95wB1rl6PrRE","outputId":"830e1522-4b98-4545-bff3-33825a26cf00"},"outputs":[],"source":["fig_tsne_label = px.scatter(tsne_results, x=0, y=1,\n","                 color=test_embeddings.label.astype(str),\n","                 symbol=test_embeddings.rotation.astype(str),\n","                 #symbol_sequence=['circle', 'cross'],\n","                 labels={'0': 'tsne-2d-one', '1': 'tsne-2d-two'})\n","fig_tsne_label.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1685108221608,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"YyPT6qu1Mihl","outputId":"c36976e0-96f9-4c0b-fdb6-baabea38fc01"},"outputs":[],"source":["fig_tsne_square = px.scatter(tsne_results, x=0, y=1,\n","                 color=test_embeddings.square.astype(str),\n","                 #symbol=test_embeddings.label.astype(str),\n","                 #symbol_sequence=['circle', 'cross'],\n","                 labels={'0': 'tsne-2d-one', '1': 'tsne-2d-two'})\n","fig_tsne_square.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1685108221609,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"u5Sqdynv1wh9","outputId":"23349a25-d4bc-4359-aa95-d3c991ee3cfc"},"outputs":[],"source":["fig_tsne_square_count = px.scatter(tsne_results, x=0, y=1,\n","                 color=test_embeddings.square_count.astype(str),\n","                 #symbol=test_embeddings.label.astype(str),\n","                 #symbol_sequence=['circle', 'cross'],\n","                 labels={'0': 'tsne-2d-one', '1': 'tsne-2d-two'})\n","fig_tsne_square_count.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1685108221609,"user":{"displayName":"Melissa Ensmenger","userId":"00774751299250409909"},"user_tz":-120},"id":"bI5_UsfZ1xET","outputId":"f38db605-4d31-4599-b8c7-ba7100fdefbf"},"outputs":[],"source":["fig_tsne_seq = px.scatter(tsne_results, x=0, y=1,\n","                 color=test_embeddings.seq_len.astype(str),\n","                 #symbol=test_embeddings.label.astype(str),\n","                 #symbol_sequence=['circle', 'cross'],\n","                 labels={'0': 'tsne-2d-one', '1': 'tsne-2d-two'})\n","fig_tsne_seq.show()"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"0c97836d00c9453aa0b4c9550d2085aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23d337edacc54aef839cbb1988d3bcd8","placeholder":"​","style":"IPY_MODEL_99ac8f1f218646e4b7eea98cb3e5326b","value":"2.021 MB of 2.033 MB uploaded (0.000 MB deduped)\r"}},"11e56769720449e0bc725660918af4d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7a60c5e3989464abe280efe80262db1","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_399994c863304434b689dfdf5f5c91e5","value":0.9944700188414392}},"23d337edacc54aef839cbb1988d3bcd8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f066c4803004cefa878105bee8596cc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"399994c863304434b689dfdf5f5c91e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"85f1b09119fe44b7a03f61b75a317dd4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_0c97836d00c9453aa0b4c9550d2085aa","IPY_MODEL_11e56769720449e0bc725660918af4d4"],"layout":"IPY_MODEL_2f066c4803004cefa878105bee8596cc"}},"99ac8f1f218646e4b7eea98cb3e5326b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7a60c5e3989464abe280efe80262db1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
